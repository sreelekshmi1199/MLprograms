{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreelekshmi1199/MLprograms/blob/main/Part_of_Speech_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d572f74",
      "metadata": {
        "id": "8d572f74"
      },
      "source": [
        "#  Part of Speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb92459",
      "metadata": {
        "id": "2eb92459"
      },
      "source": [
        "POS tagging is a task of labelling each word in a sentence with its appropriate part of speech. Some of the parts of speech include nouns, verb, adverbs, adjectives, pronouns, conjunction and their sub-categories."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f69c99",
      "metadata": {
        "id": "68f69c99"
      },
      "source": [
        "Algortihm\n",
        "\n",
        "Step 1:Start\n",
        "\n",
        "Step2:Import the required header files\n",
        "    import nltk \n",
        "        NLTK is a set of libraries for Natural Language Processing. It is a platform for building Python programs to process natural language\n",
        "    from nltk.corpus import state_union\n",
        "        The NLTK corpus is a massive dump of all kinds of natural language data sets.\n",
        "    from nltk.tokenize import PunktSentenceTokenizer\n",
        "        It is a new sentence tokenizer. This tokenizer is capable of unsupervised machine learning, so you can actually train it on any body of text that you use. \n",
        " \n",
        "Step 3:  create our training and testing data\n",
        "    One is a State of the Union address from 2005, and the other is from 2006 from past President George W. Bush.\n",
        "    train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "    sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "Step 4:  train the Punkt tokenizer\n",
        "    custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "    \n",
        "Step 5: Creating a function that will run through and tag all of the parts of speech per sentence .\n",
        "   \n",
        "   \n",
        "   def process_content():\n",
        "    try:\n",
        "        for i in tokenized[:5]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "\n",
        "process_content()\n",
        "\n",
        "Step 6:Stop\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0822b8a6",
      "metadata": {
        "id": "0822b8a6"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('state_union')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7762a40",
      "metadata": {
        "id": "e7762a40"
      },
      "source": [
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970006dc",
      "metadata": {
        "id": "970006dc"
      },
      "outputs": [],
      "source": [
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e061a16c",
      "metadata": {
        "id": "e061a16c"
      },
      "outputs": [],
      "source": [
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a5ba13",
      "metadata": {
        "id": "b2a5ba13",
        "outputId": "cf75ebce-3380-472d-ec9f-03347c5ad31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
            "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[:5]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "\n",
        "process_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19a7409",
      "metadata": {
        "id": "e19a7409"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9729008",
      "metadata": {
        "id": "f9729008"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9eb87d",
      "metadata": {
        "id": "4d9eb87d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Part of Speech tagging.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}